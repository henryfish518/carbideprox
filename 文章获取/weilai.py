import requests
from bs4 import BeautifulSoup
import os
import time
from urllib.parse import urljoin, urlparse

# -------------------------- æ ¸å¿ƒé…ç½®ï¼ˆä»…ä¿®æ”¹é€‰æ‹©å™¨ï¼Œå…¶ä½™ä¸å˜ï¼‰--------------------------
TARGET_DOMAIN = "https://www.radiancecn.com"
ARTICLE_LIST_URL = "https://www.radiancecn.com/products"
# ç²¾å‡†é€‚é…æ–°ç½‘ç«™çš„äº§å“é“¾æ¥é€‰æ‹©å™¨ï¼ˆå·²éªŒè¯å¯å‘½ä¸­ï¼‰
ARTICLE_LINK_SELECTOR = ".col-md-4 a[href*='/products/'], .product-box a, .grid-item a[href*='/products/']"
TITLE_SELECTOR = "h1, .product-title, .single-product-title"
CONTENT_SELECTOR = ".product-content, .product-detail, .single-product-content, .content"
SAVE_FOLDER = "radiancecn_articles_md"
TIMEOUT = 15
DELAY = 2
COOKIES = {}

# -------------------------- è¯·æ±‚å‡½æ•°ï¼ˆä¸å˜ï¼‰--------------------------
def get_page_html(url):
    headers = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/128.0.0.0 Safari/537.36",
        "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.7",
        "Accept-Language": "zh-CN,zh;q=0.9,en;q=0.8",
        "Accept-Encoding": "gzip, deflate, br",
        "Referer": TARGET_DOMAIN,
        "Connection": "keep-alive",
        "Upgrade-Insecure-Requests": "1",
        "Cache-Control": "max-age=0",
        "Sec-Fetch-Dest": "document",
        "Sec-Fetch-Mode": "navigate",
        "Sec-Fetch-Site": "same-origin",
        "Sec-Fetch-User": "?1"
    }
    try:
        session = requests.Session()
        response = session.get(
            url, 
            headers=headers, 
            cookies=COOKIES, 
            timeout=TIMEOUT,
            allow_redirects=True,
            verify=True
        )
        response.encoding = response.apparent_encoding
        if response.status_code == 200:
            return response.text
        elif response.status_code == 403:
            print(f"âš ï¸  é“¾æ¥ {url} è¢«403æ‹¦æˆªï¼Œå»ºè®®æ›´æ¢æ‰‹æœºçƒ­ç‚¹åé‡è¯•")
            return None
        else:
            print(f"è·å–å¤±è´¥ï¼š{url}ï¼ŒçŠ¶æ€ç ï¼š{response.status_code}")
            return None
    except Exception as e:
        print(f"è·å–å¼‚å¸¸ï¼š{url}ï¼Œé”™è¯¯ï¼š{str(e)}")
        return None

# -------------------------- å·¥å…·å‡½æ•°ï¼ˆæ–°å¢é€‰æ‹©å™¨è°ƒè¯•è¾“å‡ºï¼‰--------------------------
def create_save_folder():
    if not os.path.exists(SAVE_FOLDER):
        os.makedirs(SAVE_FOLDER)
    print(f"ä¿å­˜æ–‡ä»¶å¤¹ï¼š{os.path.abspath(SAVE_FOLDER)}")

def extract_article_links(list_html):
    soup = BeautifulSoup(list_html, "html.parser")
    # è°ƒè¯•è¾“å‡ºï¼šæŸ¥çœ‹åŒ¹é…åˆ°çš„æ ‡ç­¾æ•°é‡
    link_tags = soup.select(ARTICLE_LINK_SELECTOR)
    print(f"è°ƒè¯•ï¼šåŒ¹é…åˆ° {len(link_tags)} ä¸ªå¯èƒ½çš„é“¾æ¥æ ‡ç­¾")
    
    article_links = []
    for tag in link_tags:
        href = tag.get("href")
        if href and "/products/" in href:
            full_url = urljoin(TARGET_DOMAIN, href)
            if urlparse(full_url).netloc == urlparse(TARGET_DOMAIN).netloc:
                article_links.append(full_url)
                # è°ƒè¯•è¾“å‡ºï¼šæ˜¾ç¤ºæå–åˆ°çš„é“¾æ¥
                print(f"è°ƒè¯•ï¼šæå–åˆ°äº§å“é“¾æ¥ï¼š{full_url}")
    
    article_links = list(set(article_links))
    print(f"æœ€ç»ˆæå–åˆ° {len(article_links)} ä¸ªäº§å“è¯¦æƒ…é¡µé“¾æ¥")
    return article_links

# -------------------------- å…¶ä½™å·¥å…·å‡½æ•°ï¼ˆä¸å˜ï¼‰--------------------------
def extract_article_content(article_html):
    soup = BeautifulSoup(article_html, "html.parser")
    title_tag = soup.select_one(TITLE_SELECTOR)
    title = title_tag.get_text(strip=True) if title_tag else f"æœªå‘½åäº§å“_{int(time.time())}"
    invalid_chars = ['/', '\\', ':', '*', '?', '"', '<', '>', '|']
    for char in invalid_chars:
        title = title.replace(char, '_')
    
    content_tag = soup.select_one(CONTENT_SELECTOR)
    content_md = ""
    if content_tag:
        for p in content_tag.find_all("p"):
            p_text = p.get_text(strip=True)
            if p_text and len(p_text) > 5:
                content_md += f"{p_text}\n\n"
        for table in content_tag.find_all("table"):
            ths = table.find_all("th")
            if ths:
                header = "| " + " | ".join([th.get_text(strip=True) for th in ths]) + " |"
                separator = "| " + " | ".join(["---"] * len(ths)) + " |"
                content_md += f"{header}\n{separator}\n"
                for tr in table.find_all("tr")[1:]:
                    tds = tr.find_all("td")
                    row = "| " + " | ".join([td.get_text(strip=True) for td in tds]) + " |"
                    content_md += f"{row}\n"
                content_md += "\n"
        for img in content_tag.find_all("img"):
            img_src = img.get("src") or img.get("data-src") or img.get("data-original")
            img_alt = img.get("alt", title + "äº§å“å›¾ç‰‡")
            if img_src and "http" in img_src:
                content_md += f"![{img_alt}]({img_src})\n\n"
            elif img_src:
                full_img_src = urljoin(TARGET_DOMAIN, img_src)
                content_md += f"![{img_alt}]({full_img_src})\n\n"
        for ul in content_tag.find_all("ul"):
            for li in ul.find_all("li"):
                li_text = li.get_text(strip=True)
                if li_text:
                    content_md += f"- {li_text}\n"
            content_md += "\n"
    else:
        content_md = "æœªæå–åˆ°äº§å“è¯¦æƒ…ï¼ˆå¯è”ç³»è°ƒæ•´CONTENT_SELECTORï¼‰"
    
    return title, content_md

def save_article_to_md(title, content_md):
    file_name = f"{title}.md"
    file_path = os.path.join(SAVE_FOLDER, file_name)
    try:
        with open(file_path, "w", encoding="utf-8") as f:
            f.write(f"# {title}\n\n")
            f.write(f"> æ¥æºï¼š{TARGET_DOMAIN}\n\n")
            f.write("## äº§å“è¯¦æƒ…\n\n")
            f.write(content_md)
        print(f"âœ… ä¿å­˜æˆåŠŸï¼š{file_name}")
    except Exception as e:
        print(f"âŒ ä¿å­˜å¤±è´¥ï¼š{file_name}ï¼Œé”™è¯¯ï¼š{str(e)}")

# -------------------------- ä¸»æµç¨‹ï¼ˆä¸å˜ï¼‰--------------------------
def main():
    print(f"å¼€å§‹çˆ¬å– {TARGET_DOMAIN} äº§å“ä¿¡æ¯å¹¶ä¿å­˜ä¸ºMDæ ¼å¼...")
    create_save_folder()
    
    list_html = get_page_html(ARTICLE_LIST_URL)
    if not list_html:
        print("\nâŒ æ— æ³•è·å–äº§å“åˆ—è¡¨é¡µï¼Œçˆ¬å–ç»ˆæ­¢")
        return
    
    article_links = extract_article_links(list_html)
    if not article_links:
        print("\nâŒ æœªæå–åˆ°äº§å“é“¾æ¥ï¼Œå»ºè®®æ£€æŸ¥é€‰æ‹©å™¨æˆ–é¡µé¢æ˜¯å¦æœ‰äº§å“")
        return
    
    for idx, link in enumerate(article_links, 1):
        print(f"\n[{idx}/{len(article_links)}] æ­£åœ¨çˆ¬å–ï¼š{link}")
        article_html = get_page_html(link)
        if not article_html:
            time.sleep(DELAY)
            continue
        title, content_md = extract_article_content(article_html)
        save_article_to_md(title, content_md)
        time.sleep(DELAY)
    
    print(f"\nğŸ‰ çˆ¬å–å®Œæˆï¼æ‰€æœ‰äº§å“ä¿¡æ¯å·²ä¿å­˜åˆ° {SAVE_FOLDER} æ–‡ä»¶å¤¹")

if __name__ == "__main__":
    main()